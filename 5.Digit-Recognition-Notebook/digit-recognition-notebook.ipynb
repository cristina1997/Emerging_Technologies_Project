{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## [Introduction](https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-recognize-handwritten-digits-with-tensorflow)\n",
    "### [Neural networks](http://neuralnetworksanddeeplearning.com)\n",
    "Neural networks are a method of deep learning used to replicate how a brain functions. The neurons simulated are connected in layers and they have weights that show how they respond to signals that pass through the network.\n",
    "\n",
    "Neural networks and deep learning are used to recognise handwritten images based on observational data such as MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## [Configuration](https://corochann.com/mnist-dataset-introduction-1138.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "They are they key for the handwritten digit recognition program to work.\n",
    "\n",
    "1. Create a folder called \"tensorflow-demo\" and access it\n",
    "2. Libraries with specific versions need to be installed\n",
    "    * *Image library* - version 1.5.20\n",
    "    * *Numpy library* - version 1.14.3\n",
    "    * *Tensorflow library* - version 1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## [MNIST Dataset](https://corochann.com/mnist-dataset-introduction-1138.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The MNIST (Modified National Institute of Standards and Technology) database is a large database of handwritten digits used for \"classification\", \"image recognition\" task. It is also often used to compare algorithm performances in research.\n",
    "The dataset is made up of images of handwritten digits from 0-9 with a scale of 28x28 pixels.\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "**The tensorflow library and MNIST dataset need to be imported in the program**\n",
    "* ***Code***: \n",
    "    import tensorflow as tf \n",
    "* ***Code***: \n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "**stored in a variable - called mnist in this case - and saved in a folder - called MNIST_data (you can change the name of the folder if you wish)**\n",
    "* ***Code***: \n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "#### One-hot-encoding\n",
    "It uses a vector to represent the labels from the MNIST dataset. Because the labels are made up of digits from 0 to 9, our vector that represents those digits also contains 10 values represented in binary code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognising the digits\n",
    "#### How are the digits recognised by the binary code?\n",
    "The way this works is by having 1 out of the 10 binary values represent the handwritten digit while the other values remain 0.\n",
    "For example, the following vector [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] represents number 5 as it is in the position of where 5 would be, provided it were a list of decimal numbers from 0-10 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the visual side of the images\n",
    "In order to represent the images the 28x28 pixels are placed into a 1D vector of 784 pixels. Each of the 784 pixels is stored as a value between 0 and 255 that represents the picture's grayscale. All the pictures in this dataset are black and white, a black pixel represented by 255, a white pixel by 0 and grey shades in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets\n",
    "#### Dataset has been split into three subsets\n",
    "1. Training images - 55,000 images\n",
    "    * ***Code***: \n",
    "        n_train = mnist.train.num_examples \n",
    "2. Validation images - 5,000 images\n",
    "    * ***Code***: \n",
    "        n_validation = mnist.validation.num_examples\n",
    "3. Testing images - 10,000 images\n",
    "    * ***Code***: \n",
    "        n_test = mnist.test.num_examples\n",
    "\n",
    "We can find out the size of the dataset by looking at the num_samples on each of the above subsets and we can split the dataset into sets of images for each subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## [Neural Network Architecture](https://www.dspguide.com/ch26/2.htm)\n",
    "### Layers\n",
    "This neural network is formed in three layers\n",
    "1. Input layer - the image containing the handwritten image (28x28 pixels)\n",
    "    * ***Code***: \n",
    "        n_input = 784\n",
    "2. Hidden layer - everything between the input and output\n",
    "    * ***Code***: \n",
    "        n_hidden1 = 512\n",
    "3. Output layer - the number predicted to the user (between 0 and 9)\n",
    "    * ***Code***: \n",
    "        n_output = 10\n",
    "    \n",
    "Each layer consists of one or more nodes. The lines between the nodes are the flow of information. In our case, the information only flows from input to output, but in case of feedback, the information can flow bothways.\n",
    "![image](https://www.dspguide.com/graphics/F_26_5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "[Hyperparameters](https://www.quora.com/What-are-hyperparameters-in-machine-learning) are constants. <br>\n",
    "They cannot be changed or learned from the training process because of the \"high-level\" properties: \n",
    "* they are too complex \n",
    "* high learning speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The hyperparameters are the following:\n",
    "1. Learning rate - how much the parameters will adjust after each learning process\n",
    "    * ***Code***: \n",
    "        learning_rate = 1e-4\n",
    "2. Number of iterations - amount of times we go through training\n",
    "    * ***Code***: \n",
    "        n_iterations = 1000\n",
    "3. Batch size - amount of training images used\n",
    "    * ***Code***: \n",
    "        batch_size = 128\n",
    "4. Dropout variable - threshold at which some units are randomly eliminated\n",
    "    * ***Code***: \n",
    "        dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## TensorFlow Graph\n",
    "A TensorFlow graph has to be set up in order to build the network.  <br>\n",
    "TensorFlow is a framework that uses tensors - a data structure similar arrays or lists - to define and run computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "In this program we will define 3 tensors as placeholders:\n",
    "* ***Code***: \n",
    "    X = tf.placeholder(\"float\", [None, n_input])\n",
    "* ***Code***: \n",
    "    Y = tf.placeholder(\"float\", [None, n_output])\n",
    "* ***Code***: \n",
    "    keep_prob = tf.placeholder(tf.float32) \n",
    "    \n",
    "    The most important parameters are the ones that show the size of the data. <br>\n",
    "    In this case **None** is any amount of images inputed or outputed with either an **n_input** (28x28 pixel) shape or an **n_output** (10 digits). <br>\n",
    "    The last tensor controls the dropout rate and it allows us to *train* (dropout = 0.5) and *test* (dropout = 1.0) our probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "The paratemers that will be updated during training are the weight and bias values. These parameters have fixed values representing the location of the network's learning process.\n",
    "\n",
    "The values will be set to 0 at the start which means that they can adjust in either a positive or negative direction thus making sure that the model's learning ability is improved.\n",
    "* ***Code***: <br>\n",
    "    weights = { <br>\n",
    "    'w1': tf.Variable(tf.truncated_normal([n_input, n_hidden1], stddev=0.1)),<br>\n",
    "    'w2': tf.Variable(tf.truncated_normal([n_hidden1, n_hidden2], stddev=0.1)),<br>\n",
    "    ... <br>\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases\n",
    "The values used are small to make sure that the tensors activate at the starting stages.\n",
    "* ***Code***: <br>\n",
    "    biases = { <br>\n",
    "    'b1': tf.Variable(tf.constant(0.1, shape=[n_hidden1])), <br>\n",
    "    'b2': tf.Variable(tf.constant(0.1, shape=[n_hidden2])), <br>\n",
    "    ... <br>\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "These are used to manipulate the tensors.\n",
    "1. Matrix multiplication on the previous layer’s outputs and the current layer’s weights\n",
    "    * ***Code***: <br>\n",
    "        layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1']) <br>\n",
    "        layer_2 ...\n",
    "2. This last hidden layer will apply a dropout operation (using keep_prob = 0.5)\n",
    "    * ***Code***: <br>\n",
    "        layer_drop = tf.nn.dropout(layer_3, keep_prob)\n",
    "3. The last layer is a matrix multiplication between the weights' and bias' outputs\n",
    "    * ***Code***: <br>\n",
    "        output_layer = tf.matmul(layer_3, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "The lower the loss, the higher the accuracy of the prediction. <br>\n",
    "[Cross-entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) is a loss function which measures the performance of a classification model with a probability value between 0 and 1. \n",
    "![image](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
    "\n",
    "[Gradient descent](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) is an optimization algorithm that finds the values of a function's parameters and use them that minimizes the loss function. In other words, it measures the changes in your outputs in function of the changes in your inputs. How close to the local minimum the Gradient Descent is, is determined by the learning rate.\n",
    "![image](https://cdn-images-1.medium.com/max/1600/0*QwE8M4MupSdqA3M4.png)\n",
    "\n",
    "[Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) is an algorithm used to update network weights based on the data trained\n",
    "\n",
    "* ***Code***: <br>\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output_layer)) <br>\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Train\n",
    "The MNIST dataset is used to optimize the loss function. Every time the program iterates through the batch of training images from the dataset, the parameters are updated thus reducing loss which in turn results in the digits being predicted more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the accuracy\n",
    "First and foremost we have to define our method of evaluating the accuracy. <br>\n",
    "We can print out the accuracy on mini-batches of data to check the increase in accuracy while having a decrease in loss. <br>\n",
    "\n",
    "1. Compares which images are predicted correctly by looking at the predictions (Y) and labels (output_layer) and outputs a list of booleans\n",
    "    * ***Code***:\n",
    "        correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1)) <br>\n",
    "    and use this \n",
    "2. Cast the above to a list of floats and obtain the accuracy\n",
    "    * ***Code***: \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize session\n",
    "This will be fed training examples of our own in order to run the graph and determine the model's accuracy.\n",
    "* ***Code***: <br>\n",
    "    init = tf.global_variables_initializer() <br>\n",
    "    sess = tf.Session() <br>\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 mains steps to go through in order to minimize the loss function:\n",
    "1. Propagate values forward through the network\n",
    "2. Compute the loss\n",
    "3. Propagate values backward through the network\n",
    "4. Update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the session and check the accuracy of our program using a dropout rate of 1.0 to ensure all units are active.\n",
    "* ***Code***: <br>\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob:1.0}) <br>\n",
    "    print(\"\\nTest Accuracy:\", test_accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Test\n",
    "Once the images are trained, we can run the dataset and keep track of the amount of images that are predicted accurately. Those images can be used to calculate the accuracy of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "In order to be able to test images of your own, you need to import the libraries required\n",
    "* ***Code***: \n",
    "    import numpy as np\n",
    "* ***Code***:\n",
    "    from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test image\n",
    "Now, to check if the prediction works, test it on an image of your own. <br>\n",
    "**Note:** The image should have a white background and the handwritten number should be black.\n",
    "\n",
    "* ***Code***:\n",
    "    img = np.invert(Image.open(\"test_img.png\").convert('L')).ravel()\n",
    "    \n",
    "The open function of the reads in the image with a different representation as the one used when reading in the MNIST dataset. <br>\n",
    "This means that we need to convert the sample image to grayscale using the L parameter and store it as a numpy array. That image is then inverted from white background and black writing to black background and white writing. After that we flatten the array with ravel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Now that the testing is done we can finally predict our image\n",
    "* ***Code***: <br>\n",
    "    prediction = sess.run(tf.argmax(output_layer,1), feed_dict={X: [img]}) <br>\n",
    "    print (\"Prediction for test image:\", np.squeeze(prediction))\n",
    "    \n",
    "The function np.squeeze is more or less used to convert the prediction from an array to an integer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
